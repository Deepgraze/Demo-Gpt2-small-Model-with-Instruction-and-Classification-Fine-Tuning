# Demo-Gpt2-small-Model-with-Instruction-and-Classification-Fine-Tuning
Fine-Tuned GPT-2 Small for Instruction & Classification
An experimental GPT-2 small model, adapted for instruction-following and text classification tasks through fine-tuning.

This repository contains the code and resources for a fine-tuned version of the GPT-2 small model. The model was trained to handle basic instructional prompts and to perform text classification. This project serves as a practical example of fine-tuning a language model on consumer-grade hardware.

## üéØ Project Goals
Instruction Following: Adapt the base model to recognise and respond to simple, structured commands.

Text Classification: Train the model to categorise text into a predefined set of classes.

Accessible Example: Provide a clear and functional demonstration of the fine-tuning process.

## üîß Technical Details
Classification Method: The text classification fine-tuning was implemented using a spam detection approach.

Evaluation: The performance of the instruction-tuned model was evaluated by comparing its outputs against responses generated by the Llama 3 model running via Ollama.

## ‚ö†Ô∏è Performance & Limitations
This model was fine-tuned on a system with limited computational resources. As such, it should be considered experimental and is best suited for educational or demonstrational purposes.

It is built upon the GPT-2 small architecture and inherits its inherent limitations.

Performance will not be state-of-the-art. The model may generate errors or nonsensical outputs, particularly for complex tasks.

For production use cases, further training on larger datasets with more significant computational power would be necessary.
